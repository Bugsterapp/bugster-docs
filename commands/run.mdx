---
title: 'Run your Tests'
description: 'Execute your test specifications with browser automation'
icon: "compass"
---

The `bugster run` command executes your generated test specifications using browser automation, validating that your application works as expected. It runs tests against your configured application URL and reports results.

<Note>
  You must run `bugster init` and `bugster generate` before using the run command. Ensure your application is running on the configured base URL.
</Note>

## Basic Usage

```bash
bugster run
```

This command will:
1. **Load test specifications** from `.bugster/tests/` directory
2. **Launch browser automation** using Playwright
3. **Execute test steps** against your application
4. **Report results** with pass/fail status and detailed feedback

## How It Works

<Steps>
  <Step title="Test Discovery">
    Bugster scans for test files:
    - Loads all `.yaml` files from `.bugster/tests/`
    - Applies test limits and filtering (if configured)
    - Processes always-run tests first
    - Organizes execution order
  </Step>
  
  <Step title="Browser Automation">
    For each test specification:
    - Launches a browser instance (Chrome by default)
    - Navigates to your application
    - Authenticates using configured credentials
    - Executes each test step in sequence
  </Step>
  
  <Step title="Results Collection">
    After execution:
    - Records pass/fail status for each test
    - Captures failure reasons and screenshots
    - Optionally records video of test execution
    - Streams results to dashboard (if enabled)
  </Step>
</Steps>

## Command Flags and Options

### Path Targeting

```bash
bugster run [path]
```

<AccordionGroup>
  <Accordion title="path argument" icon="folder">
    **Purpose**: Run tests from specific file or directory
    
    **Formats**:
    - **File**: `bugster run auth/1_login.yaml`
    - **Directory**: `bugster run auth/`
    - **Relative paths**: Relative to `.bugster/tests/`
    
    **Examples**:
    ```bash
    # Run all tests in auth folder
    bugster run auth/
    
    # Run specific test file
    bugster run auth/1_login.yaml
    
    # Run all tests (same as no argument)
    bugster run
    ```
  </Accordion>
</AccordionGroup>

### Browser Configuration

```bash
bugster run --headless
```

<AccordionGroup>
  <Accordion title="--headless" icon="eye-slash">
    **Purpose**: Run tests without visible browser window
    
    **When to use**:
    - CI/CD pipelines
    - Server environments without GUI
    - Faster execution when debugging isn't needed
    
    **Default**: Browser window is visible for local debugging
    
    **Example**:
    ```bash
    bugster run --headless
    ```
  </Accordion>
</AccordionGroup>

### Output Control

```bash
bugster run --silent
```

<AccordionGroup>
  <Accordion title="--silent, -s" icon="volume-xmark">
    **Purpose**: Minimize console output during execution
    
    **What's hidden**:
    - Individual test step details
    - Browser automation logs
    - Progress indicators
    
    **What's shown**:
    - Final test results
    - Error messages
    - Summary statistics
    
    **Example**:
    ```bash
    bugster run --silent
    ```
  </Accordion>
</AccordionGroup>

```bash
bugster run --verbose
```

<AccordionGroup>
  <Accordion title="--verbose" icon="comment">
    **Purpose**: Show detailed execution logs and debugging information
    
    **Additional output**:
    - Individual test step execution
    - Browser automation details
    - Network requests and responses
    - Detailed error information
    
    **When to use**: Debugging test failures or understanding execution flow
    
    **Example**:
    ```bash
    bugster run --verbose
    ```
  </Accordion>
</AccordionGroup>

### Environment Configuration

```bash
bugster run --base-url https://staging.myapp.com
```

<AccordionGroup>
  <Accordion title="--base-url" icon="globe">
    **Purpose**: Override the base URL from configuration
    
    **Use cases**:
    - Testing against different environments
    - CI/CD with dynamic URLs
    - Vercel preview deployments
    - Staging environment testing
    
    **Examples**:
    ```bash
    # Test against staging
    bugster run --base-url https://staging.myapp.com
    
    # Test against Vercel preview
    bugster run --base-url https://my-app-git-feature-user.vercel.app
    
    # Test against different local port
    bugster run --base-url http://localhost:3001
    ```
  </Accordion>
</AccordionGroup>


### Test Selection and Filtering

```bash
bugster run --only-affected
```

<AccordionGroup>
  <Accordion title="--only-affected" icon="filter">
    **Purpose**: Run only tests affected by recent code changes
    
    **How it works**:
    - Analyzes git changes since last commit
    - Identifies modified pages and components
    - Maps changes to relevant test files
    - Includes always-run tests regardless
    
    **Benefits**:
    - Faster feedback in development
    - Focus on potentially broken functionality
    - Efficient CI/CD testing
    
    **Example**:
    ```bash
    bugster run --only-affected
    ```
  </Accordion>
</AccordionGroup>

### Concurrency Control

```bash
bugster run --max-concurrent 3
```

<AccordionGroup>
  <Accordion title="--max-concurrent" icon="layer-group">
    **Purpose**: Control maximum number of parallel test executions
    
    **Range**: 1 to 5 concurrent tests
    
    **Default**: 5 concurrent tests for optimal performance
    
    **Considerations**:
    - **Higher values**: Faster execution, more resource usage
    - **Lower values**: More stable, better for debugging
    - **Value 1**: Sequential execution, easiest debugging
    
    **Examples**:
    ```bash
    # Maximum parallelism
    bugster run --max-concurrent 5
    
    # Conservative approach
    bugster run --max-concurrent 2
    
    # Sequential execution for debugging
    bugster run --max-concurrent 1
    ```
  </Accordion>
</AccordionGroup>

## Combined Usage Examples

### Development Workflow

```bash
# Quick test run with debugging
bugster run auth/ --verbose --max-concurrent 1

# Silent production-like run
bugster run --headless --silent 

# Test specific feature after changes
bugster run --only-affected 
```

## Test Execution Flow

### Test Discovery and Limits

```bash
ğŸ§ª Loading tests...
ğŸ“Š Test limit applied: Running 5 out of 12 tests (limit: 5)

Distribution by folder:
ğŸ“ auth: 2 tests
ğŸ“ dashboard: 2 tests  
ğŸ“ checkout: 1 test

ğŸ¯ Always-run tests: 2 (additional to limit)
ğŸ“Š Total tests to run: 7 (regular: 5 + always-run: 2)
```

### Parallel Execution

When running multiple tests concurrently, you'll see compact status updates:

```bash
âš¡ Executing tests...
[Login Flow             ] Starting test
[User Registration      ] Starting test  
[Dashboard Overview     ] Starting test
[Login Flow             ] Finished: pass (Time: 12.3s)
[User Registration      ] Finished: pass (Time: 15.7s)
[Dashboard Overview     ] Finished: fail (Time: 8.9s)
```

### Results Summary

```bash
Test Results
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Name                â”‚ Result â”‚ Reason                  â”‚ Time (s) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Login Flow          â”‚ pass   â”‚ All steps completed     â”‚ 12.30    â”‚
â”‚ User Registration   â”‚ pass   â”‚ All steps completed     â”‚ 15.70    â”‚
â”‚ Dashboard Overview  â”‚ fail   â”‚ Element not found       â”‚ 8.90     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Test Summary
âœ… Passed: 2
âŒ Failed: 1  
ğŸ“Š Success Rate: 66.7%

Total execution time: 45.2 seconds
```

## Test Limits and Selection

### Default Test Limits

- **Free tier**: Maximum 5 tests per execution
- **Distribution**: Tests spread across feature folders
- **Always-run tests**: Execute in addition to the limit
- **Selection algorithm**: Representative tests chosen intelligently

### Test Selection Logic

<CardGroup cols={2}>
  <Card title="Folder Distribution" icon="folder-tree">
    **Algorithm**: Tests distributed evenly across folders
    
    **Example**: 15 total tests, 5 limit
    - `auth/`: 2 tests selected
    - `dashboard/`: 2 tests selected  
    - `checkout/`: 1 test selected
  </Card>
  
  <Card title="Always-Run Priority" icon="star">
    **Execution**: Always-run tests execute first
    
    **Additional**: Count separately from regular limits
    
    **Maximum**: Up to 3 always-run tests per execution
  </Card>
</CardGroup>

## Video Recording 

Tests automatically record execution for debugging:

### Video Recording

- **Location**: `.bugster/videos/{run-id}/{test-id}/`
- **Format**: WebM video files
- **Content**: Complete test execution from start to finish
- **Accessibility**: Videos uploaded to dashboard when streaming enabled

## Authentication Handling

### Credential Usage

Tests automatically handle authentication using configured credentials:

```yaml
# In your test file
credentials:
  - id: "admin"
    username: "admin@example.com"
    password: "admin123"
```

### Authentication Flow

1. **Credential selection**: Test uses appropriate credential set
2. **Login execution**: Automated login using provided credentials
3. **Session management**: Maintains authentication throughout test
4. **Logout handling**: Cleans up sessions between tests

## Best Practices

<CardGroup cols={1}>
  <Card title="Performance Optimization" icon="gauge-high">
    **Use appropriate concurrency**: Higher for fast feedback, lower for stability
    
    **Run targeted tests**: Use `--only-affected` during development
    
    **Environment-specific runs**: Use `--base-url` for different environments
    
    **Headless in CI/CD**: Always use `--headless` in automated pipelines
  </Card>
  
  <Card title="Debugging and Development" icon="bug">
    **Sequential execution**: Use `--max-concurrent 1` when debugging
    
    **Verbose output**: Add `--verbose` to understand test execution
    
    **Local URLs**: Test against `localhost` during development
    
    **Save results**: Use `--output` to analyze test patterns
  </Card>
  
</CardGroup>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Application not accessible">
    **Error**: Failed to navigate to application URL
    
    **Solutions**:
    - Verify your application is running on configured base URL
    - Check network connectivity and firewall settings
    - Use `--base-url` to override configuration if needed
    - Ensure URL includes correct protocol (http/https)
  </Accordion>
  
  <Accordion title="Authentication failures">
    **Error**: Login failed or authentication required
    
    **Solutions**:
    - Verify credentials in `.bugster/config.yaml` are correct
    - Test credentials manually in your application
    - Check if authentication flow has changed
    - Ensure test user accounts exist and are active
  </Accordion>
  
  <Accordion title="Test timeouts">
    **Error**: Test execution timeout or unresponsive browser
    
    **Solutions**:
    - Reduce `--max-concurrent` value for better stability
    - Check application performance and response times
    - Use `--verbose` to identify slow steps
    - Verify adequate system resources (CPU, memory)
  </Accordion>
  
  <Accordion title="Element not found errors">
    **Error**: UI elements not found during test execution
    
    **Solutions**:
    - Verify application UI hasn't changed significantly
    - Run `bugster update` to refresh test specifications
    - Use `--verbose` to see exact element selectors
    - Check if elements have dynamic IDs or classes
  </Accordion>
  
  <Accordion title="Network or streaming issues">
    **Error**: Failed to stream results or connect to Bugster services
    
    **Solutions**:
    - Check internet connectivity
    - Try with `--no-stream-results` to run locally only
    - Verify API key is valid and not expired
    - Check if corporate firewall blocks connections
  </Accordion>
</AccordionGroup>

## Exit Codes

The `bugster run` command uses standard exit codes:

- **Exit 0**: All tests passed successfully
- **Exit 1**: One or more tests failed or execution error occurred

This enables proper CI/CD integration where failed tests fail the build.

## Next Steps

After running tests, you can:

<CardGroup cols={2}>
  <Card title="Update Tests" icon="arrows-rotate" href="/commands/update">
    Keep tests synchronized with code changes
  </Card>
  
</CardGroup>